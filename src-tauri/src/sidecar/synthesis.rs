//! On-demand synthesis for commit messages, summaries, and queries.
//!
//! This module provides the high-level synthesis API that combines
//! storage retrieval with LLM or template-based generation.

use std::path::PathBuf;
use std::sync::Arc;

use anyhow::Result;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

use super::events::{DecisionType, EventType, SessionEvent};
use super::models::ModelManager;
use super::prompts::{self, CommitContext};
use super::storage::SidecarStorage;
use super::synthesis_llm::SynthesisLlm;

/// A draft commit message with metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CommitDraft {
    /// The full commit message
    pub message: String,
    /// Scope/category of the change (e.g., "feat", "fix", "refactor")
    pub scope: String,
    /// Files that should be included in this commit
    pub files: Vec<String>,
    /// Explanation of why this message was generated
    pub reasoning: String,
    /// System prompt used for generation (if LLM was used)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_prompt: Option<String>,
    /// User prompt used for generation (if LLM was used)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user_prompt: Option<String>,
    /// Whether this was generated by LLM (true) or template (false)
    pub llm_generated: bool,
    /// Number of events used to generate this draft
    pub event_count: usize,
    /// Number of checkpoints used
    pub checkpoint_count: usize,
}

/// Response to a history query
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HistoryResponse {
    /// The answer to the question
    pub answer: String,
    /// Event IDs that were used to answer
    pub source_events: Vec<Uuid>,
    /// Confidence score (0.0 - 1.0)
    pub confidence: f32,
    /// Whether this was generated by LLM (true) or template (false)
    pub llm_generated: bool,
}

/// Session summary
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SessionSummary {
    /// Session ID
    pub session_id: Uuid,
    /// Summary text
    pub summary: String,
    /// Key accomplishments
    pub accomplishments: Vec<String>,
    /// Files touched
    pub files_touched: Vec<PathBuf>,
    /// Event count
    pub event_count: usize,
    /// Whether this was generated by LLM (true) or template (false)
    pub llm_generated: bool,
}

/// Synthesizer for generating outputs from captured context
pub struct Synthesizer {
    /// Storage for retrieving events
    storage: Arc<SidecarStorage>,
    /// Model manager for embeddings (vector search)
    model_manager: Arc<ModelManager>,
    /// LLM backend for synthesis
    llm: Arc<dyn SynthesisLlm>,
    /// Whether LLM synthesis is enabled
    llm_enabled: bool,
}

impl Synthesizer {
    /// Create a new synthesizer
    pub fn new(
        storage: Arc<SidecarStorage>,
        model_manager: Arc<ModelManager>,
        llm: Arc<dyn SynthesisLlm>,
        llm_enabled: bool,
    ) -> Self {
        Self {
            storage,
            model_manager,
            llm,
            llm_enabled,
        }
    }

    /// Create a synthesizer with local LLM (legacy constructor for compatibility)
    /// Only available when the `local-llm` feature is enabled.
    #[cfg(feature = "local-llm")]
    pub fn with_local_llm(
        storage: Arc<SidecarStorage>,
        model_manager: Arc<ModelManager>,
        llm_enabled: bool,
    ) -> Self {
        let llm = Arc::new(super::synthesis_llm::LocalLlm::new(model_manager.clone()));
        Self {
            storage,
            model_manager,
            llm,
            llm_enabled,
        }
    }

    /// Generate a commit message for a session
    pub async fn synthesize_commit(&self, session_id: Uuid) -> Result<CommitDraft> {
        tracing::info!(
            "[sidecar-synthesis] synthesize_commit called for session: {}",
            session_id
        );

        // Retrieve session events and checkpoints
        let events = self.storage.get_session_events(session_id).await?;
        let checkpoints = self.storage.get_session_checkpoints(session_id).await?;

        tracing::info!(
            "[sidecar-synthesis] Retrieved {} events and {} checkpoints for session {}",
            events.len(),
            checkpoints.len(),
            session_id
        );

        if events.is_empty() {
            return Ok(CommitDraft {
                message: "chore: update".to_string(),
                scope: "chore".to_string(),
                files: vec![],
                reasoning: "No events found for this session".to_string(),
                system_prompt: None,
                user_prompt: None,
                llm_generated: false,
                event_count: 0,
                checkpoint_count: 0,
            });
        }

        // Extract files changed
        let files: Vec<PathBuf> = events
            .iter()
            .flat_map(|e| e.files_modified.clone())
            .collect::<std::collections::HashSet<_>>()
            .into_iter()
            .collect();

        // Extract decisions/reasoning
        let decisions: Vec<String> = events
            .iter()
            .filter_map(|e| match &e.event_type {
                EventType::AgentReasoning { content, .. } => Some(content.clone()),
                _ => None,
            })
            .take(5)
            .collect();

        // Get initial request from first event
        let initial_request = events
            .iter()
            .find_map(|e| match &e.event_type {
                EventType::UserPrompt { intent } => Some(intent.clone()),
                EventType::SessionStart { initial_request } => Some(initial_request.clone()),
                _ => None,
            })
            .unwrap_or_else(|| "Update code".to_string());

        // Build session summary from checkpoints or events
        let session_summary = if !checkpoints.is_empty() {
            checkpoints
                .iter()
                .map(|c| c.summary.clone())
                .collect::<Vec<_>>()
                .join(". ")
        } else {
            self.summarize_events_brief(&events)
        };

        // Try LLM generation if enabled
        tracing::debug!(
            "[sidecar-synthesis] LLM check: enabled={}, available={}, backend={}",
            self.llm_enabled,
            self.llm.is_available(),
            self.llm.description()
        );

        // Build context for LLM
        let context = CommitContext {
            session_summary: session_summary.clone(),
            files_changed: files.iter().map(|p| p.display().to_string()).collect(),
            decisions: decisions.clone(),
            initial_request: initial_request.clone(),
        };

        // Generate prompts for potential LLM use (also returned in response for visibility)
        let (system_prompt, user_prompt) = prompts::commit_message_chat(&context);

        if self.llm_enabled && self.llm.is_available() {
            tracing::debug!(
                "[sidecar-synthesis] Generating commit with LLM chat template, system_len={}, user_len={}",
                system_prompt.len(),
                user_prompt.len()
            );

            match self.llm.generate_chat(&system_prompt, &user_prompt, 256).await {
                Ok(response) => {
                    tracing::debug!(
                        "[sidecar-synthesis] LLM response: {:?}",
                        truncate(&response, 200)
                    );
                    if let Some(mut draft) = self.parse_commit_response(&response, &files) {
                        tracing::info!("[sidecar-synthesis] Successfully generated commit with LLM");
                        // Add prompt details and metadata
                        draft.system_prompt = Some(system_prompt);
                        draft.user_prompt = Some(user_prompt);
                        draft.llm_generated = true;
                        draft.event_count = events.len();
                        draft.checkpoint_count = checkpoints.len();
                        return Ok(draft);
                    } else {
                        tracing::warn!(
                            "[sidecar-synthesis] Failed to parse LLM response, falling back to template"
                        );
                    }
                }
                Err(e) => {
                    tracing::error!("[sidecar-synthesis] LLM generation failed: {}", e);
                }
            }
        } else {
            tracing::debug!("[sidecar-synthesis] Skipping LLM (not enabled or not available)");
        }

        // Fallback to template-based generation
        let file_strings: Vec<String> = files.iter().map(|p| p.display().to_string()).collect();

        let message =
            prompts::template_commit_message(&file_strings, events.len(), Some(&initial_request));

        // Extract scope from message (e.g., "feat: add foo" -> "feat")
        let scope = message
            .split(':')
            .next()
            .unwrap_or("chore")
            .to_string();

        tracing::info!(
            "[sidecar-synthesis] Generated template commit: scope='{}', files={}",
            scope,
            file_strings.len()
        );

        Ok(CommitDraft {
            message,
            scope,
            files: file_strings,
            reasoning: format!(
                "Generated from {} events and {} checkpoints (template-based)",
                events.len(),
                checkpoints.len()
            ),
            // Include prompts even for template mode so user can see what would be sent
            system_prompt: Some(system_prompt),
            user_prompt: Some(user_prompt),
            llm_generated: false,
            event_count: events.len(),
            checkpoint_count: checkpoints.len(),
        })
    }

    /// Generate a session summary
    pub async fn synthesize_summary(&self, session_id: Uuid) -> Result<SessionSummary> {
        let events = self.storage.get_session_events(session_id).await?;
        let checkpoints = self.storage.get_session_checkpoints(session_id).await?;

        if events.is_empty() {
            return Ok(SessionSummary {
                session_id,
                summary: "No events recorded".to_string(),
                accomplishments: vec![],
                files_touched: vec![],
                event_count: 0,
                llm_generated: false,
            });
        }

        // Extract files
        let files_touched: Vec<PathBuf> = events
            .iter()
            .flat_map(|e| e.files_modified.clone())
            .collect::<std::collections::HashSet<_>>()
            .into_iter()
            .collect();

        // Get initial request
        let initial_request = events
            .iter()
            .find_map(|e| match &e.event_type {
                EventType::UserPrompt { intent } => Some(intent.clone()),
                EventType::SessionStart { initial_request } => Some(initial_request.clone()),
                _ => None,
            })
            .unwrap_or_else(|| "Coding session".to_string());

        // Try LLM generation
        if self.llm_enabled && self.llm.is_available() {
            let events_summary = self.summarize_events_for_llm(&events);

            if let Ok(response) = self
                .llm
                .generate_chat(
                    "You are a helpful assistant that summarizes coding sessions.",
                    &prompts::session_summary(&events_summary, &initial_request),
                    512,
                )
                .await
            {
                let accomplishments = self.parse_bullet_points(&response);
                return Ok(SessionSummary {
                    session_id,
                    summary: response,
                    accomplishments,
                    files_touched,
                    event_count: events.len(),
                    llm_generated: true,
                });
            }
        }

        // Fallback to template
        let file_strings: Vec<String> = files_touched
            .iter()
            .map(|p| p.display().to_string())
            .collect();

        let summary = prompts::template_session_summary(
            &file_strings,
            events.len(),
            checkpoints.len(),
            &initial_request,
        );

        let accomplishments = self.extract_accomplishments(&events);

        Ok(SessionSummary {
            session_id,
            summary,
            accomplishments,
            files_touched,
            event_count: events.len(),
            llm_generated: false,
        })
    }

    /// Answer a question about session history
    pub async fn query_history(&self, question: &str, limit: usize) -> Result<HistoryResponse> {
        // Try vector search first if embeddings are available
        let events = if self.model_manager.embedding_available() {
            // Generate query embedding
            match self.model_manager.embed_one(question) {
                Ok(query_embedding) => {
                    // Try hybrid search (vector + keyword)
                    self.storage
                        .search_events_hybrid(&query_embedding, question, limit)
                        .await
                        .unwrap_or_else(|_| vec![])
                }
                Err(_) => {
                    // Fall back to keyword search
                    self.storage.search_events_keyword(question, limit).await?
                }
            }
        } else {
            // No embeddings, use keyword search
            self.storage.search_events_keyword(question, limit).await?
        };

        if events.is_empty() {
            return Ok(HistoryResponse {
                answer: "No relevant events found for this question.".to_string(),
                source_events: vec![],
                confidence: 0.0,
                llm_generated: false,
            });
        }

        let source_events: Vec<Uuid> = events.iter().map(|e| e.id).collect();

        // Try LLM generation
        if self.llm_enabled && self.llm.is_available() {
            let context = self.format_events_for_query(&events);

            if let Ok(response) = self
                .llm
                .generate_chat(
                    "You are a helpful assistant that answers questions about coding history.",
                    &prompts::history_query(question, &context),
                    256,
                )
                .await
            {
                return Ok(HistoryResponse {
                    answer: response,
                    source_events,
                    confidence: self.estimate_confidence(&events, question),
                    llm_generated: true,
                });
            }
        }

        // Fallback to showing relevant events
        let answer = format!(
            "Found {} relevant event(s):\n\n{}",
            events.len(),
            events
                .iter()
                .take(5)
                .map(|e| format!("• {}", e.content))
                .collect::<Vec<_>>()
                .join("\n")
        );

        Ok(HistoryResponse {
            answer,
            source_events,
            confidence: 0.5,
            llm_generated: false,
        })
    }

    // Helper methods

    /// Summarize events briefly for commit context
    fn summarize_events_brief(&self, events: &[SessionEvent]) -> String {
        let mut parts = Vec::new();

        let file_count = events
            .iter()
            .flat_map(|e| e.files_modified.clone())
            .collect::<std::collections::HashSet<_>>()
            .len();

        if file_count > 0 {
            parts.push(format!("Modified {} file(s)", file_count));
        }

        let tool_count = events
            .iter()
            .filter(|e| matches!(e.event_type, EventType::ToolCall { .. }))
            .count();

        if tool_count > 0 {
            parts.push(format!("{} tool calls", tool_count));
        }

        if parts.is_empty() {
            format!("{} events", events.len())
        } else {
            parts.join(", ")
        }
    }

    /// Summarize events for LLM prompt
    fn summarize_events_for_llm(&self, events: &[SessionEvent]) -> String {
        events
            .iter()
            .filter(|e| e.event_type.is_high_signal())
            .take(20)
            .map(|e| format!("- {}: {}", e.event_type.name(), truncate(&e.content, 100)))
            .collect::<Vec<_>>()
            .join("\n")
    }

    /// Format events for query context
    fn format_events_for_query(&self, events: &[SessionEvent]) -> String {
        events
            .iter()
            .take(10)
            .map(|e| {
                format!(
                    "[{}] {}: {}",
                    e.timestamp.format("%Y-%m-%d %H:%M"),
                    e.event_type.name(),
                    e.content
                )
            })
            .collect::<Vec<_>>()
            .join("\n\n")
    }

    /// Parse commit response from LLM
    fn parse_commit_response(&self, response: &str, files: &[PathBuf]) -> Option<CommitDraft> {
        // Try to extract the commit message (first non-empty line or full response)
        let lines: Vec<&str> = response
            .lines()
            .map(|l| l.trim())
            .filter(|l| !l.is_empty())
            .collect();

        if lines.is_empty() {
            return None;
        }

        // Use full response as message (cleaned up)
        let message = lines.join("\n");

        // Extract scope from first line (e.g., "feat: add foo" -> "feat")
        let scope = lines[0]
            .split(':')
            .next()
            .unwrap_or("chore")
            .to_string();

        let file_strings: Vec<String> = files.iter().map(|p| p.display().to_string()).collect();

        Some(CommitDraft {
            message,
            scope,
            files: file_strings,
            reasoning: "Generated by LLM from session context".to_string(),
            // These will be filled in by the caller
            system_prompt: None,
            user_prompt: None,
            llm_generated: true,
            event_count: 0,
            checkpoint_count: 0,
        })
    }

    /// Parse bullet points from LLM response
    fn parse_bullet_points(&self, response: &str) -> Vec<String> {
        response
            .lines()
            .filter_map(|line| {
                let line = line.trim();
                if line.starts_with('•') || line.starts_with('-') || line.starts_with('*') {
                    Some(line[1..].trim().to_string())
                } else {
                    None
                }
            })
            .collect()
    }

    /// Extract accomplishments from events
    fn extract_accomplishments(&self, events: &[SessionEvent]) -> Vec<String> {
        let mut accomplishments = Vec::new();

        // Count file operations
        let file_creates = events
            .iter()
            .filter(|e| {
                matches!(
                    e.event_type,
                    EventType::FileEdit {
                        operation: super::events::FileOperation::Create,
                        ..
                    }
                )
            })
            .count();

        let file_modifies = events
            .iter()
            .filter(|e| {
                matches!(
                    e.event_type,
                    EventType::FileEdit {
                        operation: super::events::FileOperation::Modify,
                        ..
                    }
                )
            })
            .count();

        if file_creates > 0 {
            accomplishments.push(format!("Created {} new file(s)", file_creates));
        }

        if file_modifies > 0 {
            accomplishments.push(format!("Modified {} file(s)", file_modifies));
        }

        // Extract key decisions
        for event in events.iter().take(3) {
            if let EventType::AgentReasoning {
                decision_type: Some(dt),
                content,
            } = &event.event_type
            {
                let prefix = match dt {
                    DecisionType::ApproachChoice => "Chose approach:",
                    DecisionType::Tradeoff => "Made tradeoff:",
                    DecisionType::Fallback => "Fallback:",
                    DecisionType::Assumption => "Assumed:",
                };
                accomplishments.push(format!("{} {}", prefix, truncate(content, 50)));
            }
        }

        accomplishments
    }

    /// Estimate confidence based on result quality
    fn estimate_confidence(&self, events: &[SessionEvent], query: &str) -> f32 {
        if events.is_empty() {
            return 0.0;
        }

        let query_lower = query.to_lowercase();

        // Higher confidence if events mention query terms
        let matching_events = events
            .iter()
            .filter(|e| e.content.to_lowercase().contains(&query_lower))
            .count();

        let match_ratio = matching_events as f32 / events.len() as f32;

        // Base confidence + match bonus
        (0.3 + match_ratio * 0.5).min(0.95)
    }
}

/// Truncate a string
fn truncate(s: &str, max_len: usize) -> String {
    if s.chars().count() <= max_len {
        s.to_string()
    } else {
        let mut result: String = s.chars().take(max_len.saturating_sub(3)).collect();
        result.push_str("...");
        result
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use super::super::synthesis_llm::TemplateLlm;
    use tempfile::TempDir;

    async fn setup_synthesizer() -> (TempDir, Synthesizer) {
        let temp_dir = TempDir::new().unwrap();
        let storage = Arc::new(SidecarStorage::new(temp_dir.path()).await.unwrap());
        let model_manager = Arc::new(ModelManager::new(temp_dir.path().join("models")));

        // Use template LLM for tests (no actual LLM needed)
        let llm: Arc<dyn super::super::synthesis_llm::SynthesisLlm> = Arc::new(TemplateLlm);
        let synthesizer = Synthesizer::new(storage, model_manager, llm, false);

        (temp_dir, synthesizer)
    }

    #[tokio::test]
    async fn test_synthesize_commit_empty() {
        let (_temp_dir, synthesizer) = setup_synthesizer().await;

        let session_id = Uuid::new_v4();
        let draft = synthesizer.synthesize_commit(session_id).await.unwrap();

        // Empty session should return a default message
        assert!(!draft.message.is_empty());
        assert!(draft.reasoning.contains("No events found"));
    }

    #[tokio::test]
    async fn test_synthesize_commit_with_events() {
        let (temp_dir, synthesizer) = setup_synthesizer().await;

        let session_id = Uuid::new_v4();

        // Add some events
        let events = vec![
            SessionEvent::user_prompt(session_id, "Add authentication"),
            SessionEvent::file_edit(
                session_id,
                PathBuf::from("src/auth.rs"),
                super::super::events::FileOperation::Create,
                None,
            ),
        ];

        synthesizer.storage.save_events(&events).await.unwrap();

        let draft = synthesizer.synthesize_commit(session_id).await.unwrap();

        assert!(!draft.message.is_empty());
        assert!(!draft.files.is_empty());
    }

    #[tokio::test]
    async fn test_query_history_no_results() {
        let (_temp_dir, synthesizer) = setup_synthesizer().await;

        let response = synthesizer
            .query_history("nonexistent query", 10)
            .await
            .unwrap();

        assert!(response.source_events.is_empty());
        assert_eq!(response.confidence, 0.0);
    }

    #[test]
    fn test_commit_draft() {
        let draft = CommitDraft {
            message: "feat: add feature\n\nThis adds a new feature".to_string(),
            scope: "feat".to_string(),
            files: vec!["src/main.rs".to_string()],
            reasoning: "test".to_string(),
            system_prompt: Some("system".to_string()),
            user_prompt: Some("user".to_string()),
            llm_generated: true,
            event_count: 5,
            checkpoint_count: 1,
        };

        assert!(draft.message.contains("feat: add feature"));
        assert!(draft.message.contains("This adds a new feature"));
        assert_eq!(draft.scope, "feat");
        assert!(draft.llm_generated);
        assert_eq!(draft.event_count, 5);
    }
}
