[package]
name = "qbit"
version = "0.1.0"
description = "AI-powered terminal emulator"
authors = ["you"]
edition = "2021"
default-run = "qbit"

[lib]
name = "qbit_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[[bin]]
name = "qbit"
path = "src/main.rs"
required-features = ["tauri"]

[[bin]]
name = "qbit-cli"
path = "src/bin/qbit-cli.rs"
required-features = ["cli"]

[build-dependencies]
tauri-build = { version = "2", features = [], optional = true }

[dependencies]
# Tauri (optional - only for GUI app)
tauri = { version = "2", features = [], optional = true }
tauri-plugin-opener = { version = "2", optional = true }

# CLI dependencies (optional - only for CLI binary)
clap = { version = "4", features = ["derive", "env"], optional = true }
atty = { version = "0.2", optional = true }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Async runtime
tokio = { version = "1", features = ["full"] }

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Terminal
portable-pty = "0.8"
vte = "0.13"

# Utilities
uuid = { version = "1", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
dirs = "5"
parking_lot = "0.12"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
similar = "2"

# AI / LLM integration via vtcode
vtcode-core = "0.47"
vtcode-indexer = "0.47"
futures = "0.3"
dotenvy = "0.15"

# Web search
tavily = "0.2"

# Web fetching and content extraction
reqwest = { version = "0.12", features = ["json"] }
readability = "0.2"
url = "2.5"

# Anthropic on Vertex AI (custom crate)
rig-anthropic-vertex = { path = "crates/rig-anthropic-vertex" }
rig-core = "^0.23.1"

# Google Cloud authentication (for sidecar Vertex AI synthesis)
gcp_auth = "0.12"

# Graph-based workflow execution for multi-agent systems
graph-flow = "0.4"
async-trait = "0.1"

# File walking with gitignore support
ignore = "0.4"

# TOML settings file support
toml = "0.8"
toml_edit = "0.22"

# Sidecar: Vector database and embeddings
lancedb = "0.22.3"
arrow-array = "56"
arrow-schema = "56"
fastembed = "4"

# Sidecar: Local LLM inference via mistral.rs (async, auto chat templates)
# Using Metal for GPU-accelerated inference on macOS
# Optional: enable with `--features local-llm`
# TEMPORARILY DISABLED - See: https://github.com/EricLBuehler/mistral.rs/issues/XXX
# mistralrs = { git = "https://github.com/EricLBuehler/mistral.rs.git", features = ["metal"], optional = true }

# TLS crypto provider (required for rustls 0.23+)
rustls = { version = "0.23", default-features = false, features = ["ring"] }

# HTTP Server (optional - only for server feature)
axum = { version = "0.8", optional = true }
tokio-stream = { version = "0.1", optional = true }
tokio-util = { version = "0.7", optional = true }
dashmap = { version = "6", optional = true }
base64 = { version = "0.22", optional = true }
serde_yaml = "0.9.34"

[features]
default = ["tauri"]
# Tauri GUI application (mutually exclusive with cli)
tauri = ["dep:tauri", "dep:tauri-plugin-opener", "dep:tauri-build"]
# CLI binary for headless operation (mutually exclusive with tauri)
cli = ["dep:clap", "dep:atty"]
# HTTP/SSE server for evals (extends cli, adds HTTP endpoints)
server = ["cli", "dep:axum", "dep:tokio-stream", "dep:tokio-util", "dep:dashmap", "dep:base64"]
# Enable local LLM inference via mistral.rs (adds ~400MB binary size, requires Metal on macOS)
# TEMPORARILY DISABLED - mistralrs dependency commented out due to resolution conflicts
# Uncomment mistralrs dependency above and add "dep:mistralrs" to re-enable
local-llm = []

[dev-dependencies]
tempfile = "3"
proptest = "1.4"
tower = { version = "0.5", features = ["util"] }
