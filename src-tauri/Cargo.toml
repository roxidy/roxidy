[package]
name = "qbit"
version = "0.1.0"
description = "AI-powered terminal emulator"
authors = ["you"]
edition = "2021"

[lib]
name = "qbit_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[build-dependencies]
tauri-build = { version = "2", features = [] }

[dependencies]
# Tauri
tauri = { version = "2", features = [] }
tauri-plugin-opener = "2"

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Async runtime
tokio = { version = "1", features = ["full"] }

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Terminal
portable-pty = "0.8"
vte = "0.13"

# Utilities
uuid = { version = "1", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
dirs = "5"
parking_lot = "0.12"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# AI / LLM integration via vtcode
vtcode-core = "0.47"
vtcode-indexer = "0.47"
futures = "0.3"
dotenvy = "0.15"

# Web search
tavily = "0.2"

# Web fetching and content extraction
reqwest = { version = "0.12", features = ["json"] }
readability = "0.2"
url = "2.5"

# Anthropic on Vertex AI (custom crate)
rig-anthropic-vertex = { path = "crates/rig-anthropic-vertex" }
rig-core = "0.23"

# Graph-based workflow execution for multi-agent systems
graph-flow = "0.4"
async-trait = "0.1"

# File walking with gitignore support
ignore = "0.4"

# Sidecar: Vector database and embeddings
lancedb = "0.16"
arrow-array = "53"
arrow-schema = "53"
fastembed = "4"

# Sidecar: Local LLM inference
llama-cpp-2 = "0.1"

[dev-dependencies]
tempfile = "3"
proptest = "1.4"
